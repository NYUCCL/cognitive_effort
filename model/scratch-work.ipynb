{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import random\n",
    "\n",
    "random.seed(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAG4AAABuCAYAAADGWyb7AAABYWlDQ1BrQ0dDb2xvclNwYWNlRGlzcGxheVAzAAAokWNgYFJJLCjIYWFgYMjNKykKcndSiIiMUmB/yMAOhLwMYgwKicnFBY4BAT5AJQwwGhV8u8bACKIv64LMOiU1tUm1XsDXYqbw1YuvRJsw1aMArpTU4mQg/QeIU5MLikoYGBhTgGzl8pICELsDyBYpAjoKyJ4DYqdD2BtA7CQI+whYTUiQM5B9A8hWSM5IBJrB+API1klCEk9HYkPtBQFul8zigpzESoUAYwKuJQOUpFaUgGjn/ILKosz0jBIFR2AopSp45iXr6SgYGRiaMzCAwhyi+nMgOCwZxc4gxJrvMzDY7v////9uhJjXfgaGjUCdXDsRYhoWDAyC3AwMJ3YWJBYlgoWYgZgpLY2B4dNyBgbeSAYG4QtAPdHFacZGYHlGHicGBtZ7//9/VmNgYJ/MwPB3wv//vxf9//93MVDzHQaGA3kAFSFl7jXH0fsAAAFZaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA1LjQuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOnRpZmY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vdGlmZi8xLjAvIj4KICAgICAgICAgPHRpZmY6T3JpZW50YXRpb24+MTwvdGlmZjpPcmllbnRhdGlvbj4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CkzCJ1kAAAm5SURBVHgB7V1nVBVHFE4/9o4goCamIKhgQ0XUI2ABS3rv/be99wqKYqGk94CK2LGiFJWm6b2a/LAXLCiYRDZ3cOcxs3vfE9/b97bNnjPnzru7M/e73zcf88Ock1tuEY9gwGAMNAc8EQbDJODUg4FZ8M22enwnPjEQA8RtFTAkGJEGwiWg3ICB2fCeiEZG7g2+Fa8NwkBLwEHdRsXraxBsAoYLBhbCOyoYjXtcfC9eGYAB4rYLMKhgbIwyAD4BwQkDi5yIRgTMc7JGpHVmoBXUd+Y26rz+OmMU5REGXLmNCrcXWSdSOjJA3HYRBhXIVYzWEacorWBgsVK0Bg0aSJ06dcIEFK5TkKfXz9ZQWOW2MWPGSJmZmZhwJDdAL7Cibh0DiTDlBCJuO3r0qHTt2jUpJCSEeyd/u69uuZjpwQBx2yUYnDhjx46V6JOVlcW9Y74dqAdgUfM6A0mMELUCEbcdO3aM6lbrus6dO2Pi5QsS9WGgDZR16Taq3po1azDhSG6QPtDtXXUJtM8J0rBhQ+n48eNUL0ckd50T14m7zsdniLitUincuHHjHGIpJ2vXruVEZtbG+Bi7rcstZYivFcSZ26iAxHWhoaGYePm2ZtKHzftBLZXbxo8fTzVyGoXrfKgSUioZcpxziNtOnDjhVDD6grguLCyMWyvvVYDUESkNGUDdNmHCBKrNDeO6desw4UguVkOcYisFA8vgN0d8o0aN6uU2qihxXZcuXbg95D0LFbXET40YaAv7XIbBkT5x4kSqSb1jdnY2twezZ5xGWMU2DAOo206ePFlvweiHNTU1zlxXxNQTUw0YQN02adIkqsVNx/Xr1ztz3RAN8IotZAaWQ+SIJnebO26jChPXde3aldtTrrFfsK4NA/6wjepumzx5MtXA7ZiTk4MJR3JDtYFu711SoH2O4MaNG0unTp1yWzC6kLiuW7du3N5yLeE6D88ccdsVpXBTpkyh3HscN2zYgAlHcsM8xG7r5SuUomnlNqo4cV14eDgm3gFbM+9B8wGw1qtuo+Jt3LgRE47khnuA37ZLV0LnHKFNmjSRTp8+TfnWLBLXRUREcLXk2gdty76bjbeDdVUyeQ5Cp06dqplYyo02bdrkqKOoG+9mD7ZctkpBnuQtt1EBXbiu2JYKuNE06rZp06ZRjr0WhevcUItZshrm3J8t4rYzZ854TTC6MXFd9+7dudoylhIGn5giDARCTnW3TZ8+nXLr9bh582ZMOJJLQPCKlMxAKkSOuKZNm0pnz571umBsgR49enAYZEylQiWcAeK2apkkB3EzZsxgOfXJfMuWLY76CjwjcOj2zhrCbfRk9OzZExOvzN4SqbsPgpTKbTNnzqQ8+jxu3boVE47kRqrh2zeTBq1zRDVr1kw6d+6czwVjC/bq1YvDJGMst69MfOfB8FPltlmzZrEc6jLftm0bJhzJjeJbsOevdGibI8gIbqMnRbgOP5So22bPnk150z3m5uZyh4o5ZKPxluyRzWCIqCWoefPmut9tytPSu3dvTLxD9pBI3WV7SF1VCmckt1EBt2/fjglHcg+q27J+5i2laMRtFRUVlC9DxcjISEy8w9aXie8QdducOXMMJRYLZseOHZhwJPcQ35q1f70N7XFEGNltVMA+ffpwmOUevrC2VHXddYCp6m6bO3cu5cew0e6ue0c+qY7T26JFC+n8+fOGFYwF1rdvXwdupo8v686lNWcdoa1/mIZrSZg3bx7LjaHnO3fuxIQjuYetKdn1rt5VimYmt9ET1a9fP0y8r6C3W60oXkdoSuW2+fPnUz5ME3ft2oUJR3KPWFE4ldtatmwpXbhwwTSCsUCjoqIw8Sznursxty1YsIDlwlTz3bt3Y8KR3KNWct170AzXqJndRk+YE9d9Db1a4q67BxpR3W0LFy6k/Zs27tmzhzuMzOF8DOamf96HDrgGW7VqJV28eNG0grHA+/fvz/Um9/oNRFO7jrjtX6VwixYtYns39TwvLw8TjuQeh2Ha5wNAzjVmJbfRExcdHc31KPf8LURTuq4TALe026hwe/fuxYQjuSdgmO75EBBzDbVu3doydxsVjcYBAwZwvcq9m8519wJwldsWL15M+7Rc3LdvHyYcyT0JwzTPR4CUa4S47dKlS5YTjG1o4MCBXM8yB99BNMVdh7otMTGR7dGSc7O77mOl29q0aWN5t9GTOGjQIMx13wMnt8Ew7HMfIPsPBgc+KSmJ9mX56MJ1TxlWNQD2iVI04rbKykrLC8Y2OHjwYO7gypz8ANGQrrsfgKnctmTJErYnW8zz8/Mx4UjuaRiGe1Ru8/Pzs53b6MmMiYnBxPsRVDOU6x4AQCq3LV26lPZhu1hQUIAJR3LPGMlynwIYDmjbtm1t6zZ6So3uOtRtycnJFL9tY2FhIXeYmcP9rBFc9xkDqBYocdvly5dtKxjbeGxsLCbeT8CZrnddCAC4phRu2bJlLHZbz4uKijDhSO45GLo9n0NlDpi/v79wm+KoxsXFcRzJnP0MURfXdYbCwm0KkbCf+/fvx4Qjuedh+PzJhIocIOK2K1euYNhtnxsyZAjHlcwdcd3tvlQOddvy5cttL5AzAg4cOIAJR3Iv+FK4LCjGAQkICBBuc6aanB86dCjHmczhLxB94rpQKKS621JSUm4AW7w+ePAgJpzPXLdGuM39Qzhs2DBMvF+97bowzG0rVqxwvxObrSwuLsaEI7kXYXjtWQs7c4XbtWsnVVVV2Yx+z9odPnw4x6HM6W8QvXLXdYGNVXfbypUrPevChqtLSkow4UjuJRiaP8JtGh6y+Ph4TDzNXUfcVgODK7Zq1SoNW7HXVi5c97KWllunFC0wMFDcbR6eNSeu+x24vkML8brCJiq3rV692kPYYnlpaSn3F4wxxytaCJfNbFhbKCgoSKqurhbMa8BAQkICJt4fwLlHrusGG6jclpqaqgFksQVhoKysDBOO5F6F4fazHlZyGwu3aX/gRowYwXEsc+6268JhA+E27XVS7VheXo4JR3KvwbjpJwdWcBsGBweLu01FuzaJkSNHclzL3P8J8abuughYoHJbWlqaNijFLioGDh06hAlHcq/DqPezAb7kNiJuu3r1qqqgSGjHwKhRozjOZQ2OQLyzPsqhbktPT9cOodgJZeDw4cOYcCRXL9dtlJV2bNK+fXvhNpRq7ZOjR4928M7ocATmLl3XHT5Q3W0ZGRnaIxQ7ogy4cN0boI3TZxO8wRQXOf15+Qu0QV3XQ4hm+EP7JmikejZDRjjL2Bz8DRrdRZSj/wl0T5jb8v9aQUgw0dMBsHL/ciDcZmynsX8JHa4z0YETUCkD/wNh9GDMHZ7o+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(\"stimuli/stim03.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load pomcp.py\n",
    "# revisiting pomcp with the new game dynamics to compare to the real data\n",
    "\n",
    "# pamop @nyu.edu, gureckis @gmail.com\n",
    "# last edited dec 3, 2019\n",
    "\n",
    "# imports\n",
    "# from random import randint\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import math\n",
    "import copy\n",
    "import ast\n",
    "import random\n",
    "import pprint\n",
    "import datetime  # for limiting calculation to wall clock time\n",
    "import numpy as np\n",
    "import anytree as tree\n",
    "from scipy import optimize\n",
    "from ascii_graph import Pyasciigraph\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import cProfile\n",
    "import re\n",
    "\n",
    "# # in order to import the BlockGame dynamics\n",
    "# sys.path.append(\"..\")\n",
    "# from exp2.gamedynamics import BlockGame # how long has this been exp instead of exp2? feb 12 2020 ## Oh actually doesn't matter, not used in the code at all\n",
    "\n",
    "\n",
    "class POMCP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        simgame, # I am not giving this one a default because you GOTTA PUT IT IN THERE or this will NO WORK\n",
    "        observation=2,\n",
    "        gamma=0.8,\n",
    "        explore=1,\n",
    "        epsilon=1e-7,\n",
    "        n_particles=100,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        # general model parameters\n",
    "        self.gamma = gamma  # discount factor for future rewards\n",
    "        self.epsilon = epsilon  # explore for epsilon greedy\n",
    "        self.explore = explore  # explore for UCB\n",
    "        self.n_particles = n_particles\n",
    "\n",
    "        # the block game we are playing\n",
    "        self.simgame = simgame \n",
    "        self.target = self.simgame.target\n",
    "        # self.simgame = BlockGame(self.target) # i want to try this again bc i want it to work\n",
    "\n",
    "        self.noprint = kwargs.get(\"noprint\", False)\n",
    "        self.calculation_time = datetime.timedelta(seconds=kwargs.get(\"time\", 5))\n",
    "        # self.maxdepth = kwargs.get(\"maxdepth\", 20)\n",
    "        self.nsims = kwargs.get(\"nsims\", 100)\n",
    "        self.priors = kwargs.get(\"priors\")\n",
    "\n",
    "        self.choicealg = kwargs.get(\"choicealg\", \"ucb\")  # \"ucb\" \"pref\" \"egreedy\"\n",
    "\n",
    "        # decision tree and histories\n",
    "        self.tree = tree.Node(\n",
    "            \"root\",\n",
    "            N=self.N_init(),\n",
    "            V=self.V_init(),\n",
    "            belief=self.B_init(self.priors, self.n_particles),\n",
    "        )  # self.tree is the entire tree\n",
    "        self.root = self.tree  # self.root points to the current node in the tree\n",
    "        self.history = []  # history of actions and observations\n",
    "        self.last_event = None  # the last action or observation made\n",
    "\n",
    "        ## debugging variables\n",
    "        self.last_simulate_rollout_q = False  # did the last simulation use rollout?\n",
    "        self.max_depth_last_simulate = 0  # what was the depth of the last simulation\n",
    "\n",
    "        # preferred actions dataframe\n",
    "        try:\n",
    "            self.best_actions = pd.read_csv(\"model/shortestpaths.csv\",index_col=0)\n",
    "        except: # This happens when i'm running from a jupyter notebook, paths are weird in this world *eye roll* \n",
    "            # print(\"you look like you're accessing this file from a jupyter notebook\") \n",
    "            self.best_actions = pd.read_csv(\"../model/shortestpaths.csv\",index_col=0)\n",
    "\n",
    "    def print_tree_choices(self):\n",
    "        pprint.pprint(self.tree.children)\n",
    "\n",
    "    def print_root_choices(self):\n",
    "        pprint.pprint(self.root.children)\n",
    "\n",
    "    def print_root_parents(self):\n",
    "        pprint.pprint(self.root.parent.children)\n",
    "\n",
    "    def print_tree(self):\n",
    "        \"\"\"prints a representation of the tree to the terminal\"\"\"\n",
    "        print(tree.RenderTree(self.tree, style=tree.DoubleStyle))\n",
    "\n",
    "    def add_to_history(self, event):\n",
    "        self.history += [str(event)]\n",
    "        self.last_event = str(event)\n",
    "\n",
    "    @staticmethod\n",
    "    def terminal_histogram(data, bins, title=\"Particle Distribution\"):\n",
    "        \"\"\"plots a hisogram of the belief vector\"\"\"\n",
    "        heights, bars = np.histogram(data, bins)\n",
    "        bars_chart = [(str(i), j) for i, j in zip(bars, heights)]\n",
    "        graph = Pyasciigraph()\n",
    "        for line in graph.graph(title, bars_chart):\n",
    "            print(line)\n",
    "\n",
    "    @staticmethod\n",
    "    def string_path(path):\n",
    "        \"\"\"a helper function for formatting the full path to a node\"\"\"\n",
    "        return \"/\".join([\"\"] + [str(node.name) for node in path])\n",
    "\n",
    "    @staticmethod\n",
    "    def N_init():\n",
    "        \"\"\"initializes the counter of number of visits to a node\"\"\"\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def V_init():\n",
    "        \"\"\"initializes the value of a node\"\"\"\n",
    "        return 0.0\n",
    "\n",
    "    # This is a little hacky because it is specific to my experiment type - the sort of prior i give subjects\n",
    "    # is two states where each is equally likely. could generalize for diff types of priors\n",
    "    @staticmethod\n",
    "    def B_init(priors=None, n_particles=None):\n",
    "        \"\"\"initializes belief particle.  either empty list or sampled from a provided prior\"\"\"\n",
    "        if priors is None and n_particles is None:\n",
    "            return [] \n",
    "        else:\n",
    "            return random.choices(priors, k=n_particles)\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_select(values):\n",
    "        \"\"\"chooses the maximum in values, selecting randomly if there are ties\"\"\"\n",
    "        values = np.array(values)  # convert to array\n",
    "        return random.choice(np.flatnonzero(values == values.max()))\n",
    "\n",
    "    def observe(self, obs):\n",
    "        \"\"\"updates the model based on observation that was made\"\"\"\n",
    "        self.add_to_history(obs)\n",
    "        # If that observation was already simulated, choose its existing node. Otherwise create obs node\n",
    "        child = [c for c in self.root.children if c.name == str(obs)]\n",
    "        if not child:\n",
    "            print(\"No obs node for what just happened, making a new one\")\n",
    "            obs_node = tree.Node(\n",
    "                str(obs),\n",
    "                parent=self.root,\n",
    "                N=self.N_init(),\n",
    "                V=self.V_init(),\n",
    "                belief=self.B_init(),\n",
    "            )\n",
    "        else:\n",
    "            obs_node = child[0]\n",
    "        self.root = obs_node\n",
    "\n",
    "    def choose(self):\n",
    "        \"\"\"asks the agent to choose which action to take\"\"\"\n",
    "        completedsims = 0\n",
    "        while completedsims < self.nsims:\n",
    "            try:\n",
    "                sampled_state = random.choice(self.root.belief)\n",
    "            except IndexError:  # if no belief\n",
    "                print(\n",
    "                    \"Encountered an error where the belief vector is empty for this current node.  Choosing randomly for now\"\n",
    "                )\n",
    "                # print(len(self.simgame.StateNames)) # oh god it needs a -1  \n",
    "                sampled_state = random.randint(0, len(self.simgame.StateNames)-1)  # WEEEOOOO WEEEOO THIS WAS SPITTING OUT 13s 2/18/20\n",
    "            self.simulate(sampled_state, self.history, self.root, 0)\n",
    "            completedsims += 1\n",
    "            # self.explore -= 1.0 / self.nsims  # slowly decreasing explore HACKY\n",
    "            if completedsims % 99 == 0 and not self.noprint:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "        childvals = np.array(\n",
    "            [(idx, n.V) for idx, n in enumerate(self.root.children) if n.N > 0]\n",
    "        )  # what if all are zero?\n",
    "\n",
    "        bestkididx = POMCP.greedy_select(childvals[:, 1]) # HOW TO MAKE THIS IGNORE NON-PREF ACTIONS?\n",
    "        bestkid = self.root.children[int(childvals[bestkididx, 0])]\n",
    "\n",
    "        # update agent tree (step down to the action we actually take)\n",
    "        self.root = bestkid\n",
    "        # our chosen action!\n",
    "        action = bestkid.name\n",
    "        # update history bc we are going to take this action\n",
    "        self.add_to_history(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def simulate(\n",
    "        self, sampled_state, current_history, parent_node, depth, force_actions=None\n",
    "    ):  \n",
    "        # pnode = parent action node\n",
    "        if self.gamma ** depth < self.epsilon: # or depth > self.maxdepth:\n",
    "            self.max_depth_last_simulate = depth\n",
    "            return 0\n",
    "\n",
    "        # print(\"Simulating\", sampled_state)\n",
    "        # if there are no action nodes, create them\n",
    "        if len(parent_node.children) == 0:  # no current children - EXPAND\n",
    "            # expand\n",
    "            for a in self.simgame.all_actions():\n",
    "                tree.Node(\n",
    "                    str(a), parent=parent_node, N=self.N_init(), V=self.V_init()\n",
    "                )  # action nodes don't need belief\n",
    "            # act = random.choice(self.simgame.all_actions()) #RANDOM ROLLOUT\n",
    "            R = self.rollout(sampled_state, depth)  # rollout\n",
    "            return R\n",
    "\n",
    "        # Are we in the final state? please end the trial for goodness' sake\n",
    "        if sampled_state == self.target:\n",
    "            action = \"[3, 'q', 'q']\"\n",
    "            choice_node = [\n",
    "                c for c in parent_node.children if c.name == str(action)\n",
    "            ][0]\n",
    "            action_array = [3, \"q\", \"q\"]\n",
    "        else:\n",
    "            #otherwise, we will choose one of the child actions using ucb\n",
    "            # explore\n",
    "            if self.choicealg == \"egreedy\":\n",
    "                choice_node = self.e_greedy_action_selection(\n",
    "                    parent_node, self.explore, sampled_state\n",
    "                )\n",
    "            elif self.choicealg == \"ucb\":\n",
    "                choice_node = self.ucb_action_selection(\n",
    "                    parent_node, self.explore\n",
    "                )\n",
    "            elif self.choicealg == \"pref\": \n",
    "                # add another possible action selection algorithm. only choose among the nodes that \n",
    "                # are preferred actions (e.g., shortest path to target actions + questions)\n",
    "                # -- preferred actions could also end up including only \"good\" questions instead of all questions\n",
    "                choice_node = self.pref_action_selection(\n",
    "                    parent_node, self.explore, sampled_state, self.target\n",
    "                )\n",
    "            else: # default use UCB\n",
    "                choice_node = self.ucb_action_selection(\n",
    "                    parent_node, self.explore\n",
    "                )\n",
    "            \n",
    "            action = choice_node.name\n",
    "            action_array = ast.literal_eval(action)\n",
    "        # end if\n",
    "\n",
    "        # Sample a transition, observation, and reward from our generative model G\n",
    "        new_state, obs, r, done = self.simgame.G_noisy(sampled_state, action_array)\n",
    "\n",
    "        # Now we must access or create the node for the next history (current hist + action + obs)\n",
    "        new_history = current_history + [str(action)] + [str(obs)]\n",
    "        \n",
    "        # add an observation node if needed\n",
    "        child = [c for c in choice_node.children if c.name == str(obs)]\n",
    "        if not child:\n",
    "            obs_node = tree.Node(\n",
    "                str(obs),\n",
    "                parent=choice_node,\n",
    "                N=self.N_init(),\n",
    "                V=self.V_init(),\n",
    "                belief=self.B_init(),\n",
    "            )\n",
    "        else:\n",
    "            obs_node = child[0]\n",
    "\n",
    "        # *** ADDING A PARTICLE TO THE CHILD NODE BELIEF STATE: HELPFUL OR PROBLEMATIC?\n",
    "        # obs_node.belief.append(new_state)\n",
    "        # # obs_node.N += 1\n",
    "        # if len(obs_node.belief) > self.n_particles:\n",
    "        #     obs_node.belief.pop(0)\n",
    "\n",
    "        # Recurse to get total reward\n",
    "        R = r + self.gamma * self.simulate(new_state,new_history,obs_node,depth+1)\n",
    "\n",
    "        parent_node.belief.append(sampled_state) # unsure if this is a good idea anymore because it's changing the current node belief while simulating\n",
    "        if len(parent_node.belief) > self.n_particles:\n",
    "            parent_node.belief.pop(0)\n",
    "        parent_node.N += 1\n",
    "\n",
    "        choice_node.N += 1\n",
    "        choice_node.V = choice_node.V + (R - choice_node.V) / choice_node.N # max(choice_node.V, R)\n",
    "\n",
    "        return R\n",
    "\n",
    "    def rollout(self, s, depth):\n",
    "        if self.gamma ** depth < self.epsilon: # or depth > self.maxdepth:\n",
    "            self.max_depth_last_simulate = depth\n",
    "            return 0\n",
    "\n",
    "        #in the rollout function, choose the rollout action policy\n",
    "        if s==self.target:\n",
    "            a = [3, \"q\", \"q\"]\n",
    "            # Probably should end the game cuz you think you in the end goal state!\n",
    "        else: \n",
    "            # Feb 11 reimplementing todd's pref actions\n",
    "            # ask_acts = self.simgame.all_actions()[9:16]\n",
    "            actions_to_target = []\n",
    "\n",
    "            actions_to_target.append(self.retrieve_best_action(s,self.target))\n",
    "\n",
    "            for act in self.simgame.all_actions():\n",
    "                if act[0]==2: # question action\n",
    "                    # implement something about only adding good questions?\n",
    "                    actions_to_target.append(act)\n",
    "\n",
    "            a = random.choice(actions_to_target)\n",
    "        \n",
    "\n",
    "        # ask the black box \"what would happen if i took this action in this state?\"\n",
    "        s_new, o, r, done = self.simgame.G_noisy(s, a)\n",
    "\n",
    "        # if the game is over, we have reached a terminal state and stop rolling out\n",
    "        if done:\n",
    "            self.max_depth_last_simulate = depth\n",
    "            return r\n",
    "\n",
    "        # CAptains log star date jan 31 ok. I'm gonna make my own new smart rollout bc i am doing it MY WAY\n",
    "        # Let's start simple. if u in target state, end the game, if not, don't. \n",
    "        # if s_new == self.target:\n",
    "        #     # Probably should end the game cuz you think you in the end goal state!\n",
    "        #     a_new = [3, \"q\", \"q\"]\n",
    "        # else: \n",
    "        #     # let us NOT choose the end game action, but check out other possibilities. \n",
    "        #     a_new = random.choice(self.simgame.all_actions()[0:16])\n",
    "\n",
    "        # captain's log star date january 29, 2020. the heuristicy rollout below has been preventing my\n",
    "        # beautiful POMCP from being able to flourish in an environment of noisy transitions. i cry.\n",
    "        # i fix. (wish i could have some sort of preferred actions instead of just random rollout tho)\n",
    "        # a_new = random.choice(self.simgame.all_actions())\n",
    "\n",
    "        # THIS PART IS VERY SPECIFIC TO THE WORLD MODEL / GAME BEING PLAYED! NOT GENERAL IN THIS FORMAT!\n",
    "        # next action: rollout can choose dumbly (randomly) or wisely (informed) \n",
    "        # This is different now that there is stochasticity in the problem though, because \n",
    "        # the easy heuristic of \"if the simulated next state is the target then choose end action\"\n",
    "        # won't be 100% faithful like it was before.\n",
    "        # ask_acts = self.simgame.all_actions()[9:16]\n",
    "        # actions_to_target = []\n",
    "\n",
    "        # actions_to_target.append(ask_acts)\n",
    "\n",
    "        # if s_new == self.target:  # in parent_node.belief:\n",
    "        #     # WITH transition probability known of problem,\n",
    "        #     if random.random() < self.simgame.tProb:\n",
    "        #         a_new = [3, \"q\", \"q\"]\n",
    "        # else:\n",
    "        #     for a in self.simgame.all_actions():\n",
    "        #         _ns = self.simgame.next_state(s_new, a)\n",
    "        #         if _ns == self.target:\n",
    "        #             actions_to_target.append(a)\n",
    "\n",
    "        # if len(actions_to_target) > 0:\n",
    "        #     a_new = random.choice(actions_to_target)\n",
    "        # else:\n",
    "        #     a_new = random.choice(self.simgame.all_actions()) # any of the blockgame actionssssss\n",
    "\n",
    "        #     # rollout policy could be wiser - only questions, or only legal actions\n",
    "        return r + self.gamma * self.rollout(s_new, depth + 1)\n",
    "\n",
    "    def retrieve_best_action(self,state,target):\n",
    "        a = self.best_actions.loc[state,str(target)]\n",
    "        # try:\n",
    "            \n",
    "        # except:\n",
    "        #     print(\"i tried to call best actions on state \" + str(state) + \" and target \" + str(target))\n",
    "        return ast.literal_eval(a) #pandas is annoying\n",
    "\n",
    "    def pref_action_selection(self, parent, explore, state, target):\n",
    "        # prune the tree - only choose between select child nodes. --> choose greedily or UCB among child nodes? let's try ucb'\n",
    "        # use \"retrieve best action\" method to select the shortest path move action\n",
    "        # then add the \"ask\" actions as well and then do UCB over them\n",
    "\n",
    "        moveact = self.retrieve_best_action(state, target)\n",
    "        prefchildren = [child for child in parent.children if ast.literal_eval(child.name)[0]>1 or ast.literal_eval(child.name)==moveact]\n",
    "\n",
    "        # now do ucb over the children\n",
    "        sumn = np.array([c.N for c in prefchildren]).sum()\n",
    "        vals = [ UCB(parent.N + 1, c.N + 1, c.V, explore) for c in prefchildren]\n",
    "        choice = self.greedy_select(vals)\n",
    "\n",
    "        # could also choose randomly over the children instead\n",
    "\n",
    "        return prefchildren[choice]\n",
    "\n",
    "    @staticmethod\n",
    "    def ucb_action_selection(parent, explore):\n",
    "\n",
    "        sumn = np.array([child.N for child in parent.children]).sum()\n",
    "\n",
    "        vals = [ UCB(parent.N + 1, child.N + 1, child.V, explore) for child in parent.children\n",
    "        ]\n",
    "        # print(\"\\tUCB\", vals)\n",
    "        choice = POMCP.greedy_select(vals)\n",
    "        return parent.children[choice]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def e_greedy_action_selection(parent, explore, current_state):\n",
    "        # nodes = [c for c in parent.children if (BlockGame.next_state(current_state, ast.literal_eval(c.name))!=current_state or c.name[1]=='2')]\n",
    "        nodes = parent.children[:]\n",
    "        if random.random() < explore:\n",
    "            # randomly select\n",
    "            return random.choice(nodes)\n",
    "        else:\n",
    "            # choose best\n",
    "            vals = [n.V for n in nodes]\n",
    "            choice = POMCP.greedy_select(vals)\n",
    "            return nodes[choice]\n",
    "\n",
    "\n",
    "\n",
    "#UCB score calculation from George Pik pomcp code\n",
    "def UCB(N,n,V,c = 1): #N=Total, n= local, V = value, c = parameter\n",
    "    return V + c*np.sqrt(np.log(N)/n)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category learning task parameters\n",
    "nDimensions = 3\n",
    "nProblemTypes = 6\n",
    "nStimuli = 8\n",
    "\n",
    "dimensions = list(range(0,nDimensions))\n",
    "sizes = [0,1] # large, small\n",
    "shapes = [0,1] # square, triangle\n",
    "colors = [0,1] # black, white\n",
    "problemTypes = list(range(0,nProblemTypes))\n",
    "stimuli = list(range(0,nStimuli))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem types\n",
    "\n",
    "#Returns 0 for category A, 1 for category B\n",
    "def checkStimulusCategory(problemType,stim):\n",
    "    if problemType==0:\n",
    "        # Shepard type I\n",
    "        # 0  0 0 0 - 0\n",
    "        # 1  0 0 1 - 1\n",
    "        # 2  0 1 0 - 0\n",
    "        # 3  0 1 1 - 1\n",
    "        # 4  1 0 0 - 0\n",
    "        # 5  1 0 1 - 1\n",
    "        # 6  1 1 0 - 0\n",
    "        # 7  1 1 1 - 1\n",
    "        return stim % 2\n",
    "    elif problemType==1: # Shepard type II\n",
    "        labels = [0,1,1,0,0,1,1,0]\n",
    "        return(labels[stim])\n",
    "    elif problemType==2: # Shepard type III\n",
    "        labels = [1,0,1,0,1,1,0,0]\n",
    "        return(labels[stim])\n",
    "    elif problemType==3: # Shepard type IV\n",
    "        labels = [1,1,1,0,1,0,0,0]\n",
    "        return(labels[stim])\n",
    "    elif problemType==4: # Shepard type V\n",
    "        labels = [1,0,1,0,1,0,0,1]\n",
    "        return(labels[stim])\n",
    "    elif problemType==5: # Shepard type VI\n",
    "        labels = [1,0,0,1,0,1,1,0]\n",
    "        return(labels[stim])\n",
    "    else:\n",
    "        raise ValueError(\"problem type must be between 0 and 5 inclusive\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined as a POMDP\n",
    "nStates = 70\n",
    "\n",
    "states = list(range(0,nStates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded B\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded B\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded B\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded B\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded B\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded B\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded B\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is A)\n",
      "agent responded A\n",
      "press enter to continue\n",
      "A or B?\n",
      "(Note: correct response is B)\n",
      "agent responded A\n",
      "press enter to continue\n"
     ]
    }
   ],
   "source": [
    "nTrials = 32\n",
    "problemType = 0 # between 0 and 5, or could randomly choose\n",
    "\n",
    "# randomly assign size, shape, and color to x y and z axes. Randomly assign sm/lg sq/tr bl/wh to 0 1. \n",
    "dimOrder = [0,1,2] #random.sample(dimensions,nDimensions)\n",
    "sizeOrder = [0,1] #random.sample(sizes,len(sizes))\n",
    "shapeOrder = [0,1] #random.sample(shapes,len(shapes))\n",
    "colorOrder = [0,1] #random.sample(colors,len(colors))\n",
    "\n",
    "stimuliFiles = [\"stimuli/stim00.png\",\n",
    "               \"stimuli/stim01.png\",\n",
    "               \"stimuli/stim02.png\",\n",
    "               \"stimuli/stim03.png\",\n",
    "               \"stimuli/stim04.png\",\n",
    "               \"stimuli/stim05.png\",\n",
    "               \"stimuli/stim06.png\",\n",
    "               \"stimuli/stim07.png\"]\n",
    "\n",
    "agentCorrectPercent = 0.8\n",
    "options = ['A','B']\n",
    "\n",
    "for i in range(nTrials):\n",
    "    #display stimulus\n",
    "    stim = random.choice(stimuli)\n",
    "    Image(stimuliFiles[stim])\n",
    "    \n",
    "    print(\"A or B?\")\n",
    "    correctResp = checkStimulusCategory(problemType,stim)\n",
    "    print(\"(Note: correct response is \" + options[correctResp] + \")\")\n",
    "    \n",
    "    #get response\n",
    "    if random.random()<agentCorrectPercent:\n",
    "        resp = correctResp\n",
    "    else:\n",
    "        resp = 1 - correctResp\n",
    "        \n",
    "    \n",
    "    print(\"agent responded \" + options[resp])\n",
    "        \n",
    "    #move to next trial\n",
    "    input('press enter to continue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0,1]\n",
    "b = [1,0]\n",
    "\n",
    "random.shuffle(a) #random.choice([[0,1],[1,0]])\n",
    "sizes = random.sample(b, len(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.597695785436328"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-53a44a914c75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "for a in range(100):\n",
    "    print(a)\n",
    "    time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTestTrials = 8 # IMPORTANT!! !IF THIS NUMBER CHANGES, CODE BELOW NEEDS TO CHANGE\n",
    "        nHits=0\n",
    "\n",
    "        for record in user_data['data']: # for line in data file\n",
    "            trial = record['trialdata'] # get part of line holding trial info\n",
    "            if trial['phase'] == 'test':\n",
    "                if trial['hitormiss'] == True:\n",
    "                    nHits+=1\n",
    "\n",
    "        incentive = trial['incentive'] #number of points for above chance correct responses\n",
    "        maxreward = 10.00\n",
    "\n",
    "        # point multiplier is based on subject performance above chance\n",
    "        if nHits <= 4: # if subject got at or less than half correct, at or worse than chance\n",
    "            pointMult=0\n",
    "        else:\n",
    "            pointMult=nHits-4\n",
    "\n",
    "        # incentive level times point multiplier\n",
    "        if incentive==0:\n",
    "            bonus_prob = 0\n",
    "        else:\n",
    "            n = np.log2(incentive)\n",
    "            maxprob = 1/(2**5-n)\n",
    "            bonus_prob = maxprob * pointMult/4\n",
    "\n",
    "        #use probability to calculate whether they get $10 by pulling a random number between 0 and 1\n",
    "        if np.random.random() < bonus_prob: # NOTE: if i show them this during task, np.random will be called before and the number saved to data\n",
    "            reward=10.00\n",
    "        else:\n",
    "            reward=0.00\n",
    "\n",
    "        user.bonus = reward"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
